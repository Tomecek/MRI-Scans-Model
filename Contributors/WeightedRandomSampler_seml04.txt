# I worked with BaselineCNN model from Tomáš.
# The baseline model BaselineCNN achieved very good results with an accuracy of 97.50%.
# I experimented with various approaches to improve the model.

# First, I tried incorporating light data augmentation.
# After training the model, the results were worse (accuracy 85%), and the model performed poorly in classifying the classes.
# I abandoned augmentation because the dataset contains MRI images that are standardized.
# For example, rotations or horizontal flips can introduce unnatural variations.
# This can confuse the model with deformations that do not occur in real data.

# Next, I focused on addressing class imbalance, as the classes in the dataset were unbalanced:
# 1. First, I tried adding a Weighted Loss function to penalize errors on underrepresented classes.
#    - After training, the model achieved an accuracy of 49.53% and misclassified all samples as class 2.
# 2. Then I tried oversampling using a WeightedRandomSampler.
#    - The results were better, with an accuracy of 96.48%. The model classified the underrepresented classes better.
#    - Still, the accuracy was slightly lower than the baseline model.

# Next, I tested the combination of Weighted Loss and an optimizer:
# - Here, the accuracy dropped to 95.08%.

# I tried oversampling again (accuracy 96.09%) and finally oversampling
# with minor fine-tuning at the end, but this reduced the accuracy to 95.94%.

# Here is the code I used for oversampling with WeightedRandomSampler, which yielded the best results:

N_CLASSES = df_train['label'].nunique()  # Number of classes in the dataset


class ImageDataset(Dataset):
    def __init__(self, dataframe):
        self.dataframe = dataframe
       
    def __len__(self):
        return len(self.dataframe)  # Dataset length
   
    def __getitem__(self, idx):
        image = self.dataframe.iloc[idx]["img_arr"]  # Retrieve image as numpy array
        label = self.dataframe.iloc[idx]["label"]    # Retrieve label
        # CNN model requires adding channel dimension: (128, 128) -> (1, 128, 128)
        image = torch.tensor(image, dtype=torch.float32).unsqueeze(0)
        label = torch.tensor(label, dtype=torch.long)
        return image, label

# Define the CNN model
class BaselineCNN(nn.Module):
    def __init__(self):
        super(BaselineCNN, self).__init__()
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)
        self.pool1 = nn.MaxPool2d(2, 2)
        self.batchnorm1 = nn.BatchNorm2d(num_features=32)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.pool2 = nn.MaxPool2d(2, 2)
        self.flatten = nn.Flatten()
        self.fc1 = nn.Linear(64 * 32 * 32, 128)
        self.out = nn.Linear(128, N_CLASSES)
       
    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = self.pool1(x)
        x = self.batchnorm1(x)
        x = F.relu(self.conv2(x))
        x = self.pool2(x)
        x = self.flatten(x)
        x = F.relu(self.fc1(x))
        x = self.out(x)
        return x

# Set hyperparameters
learning_rate = 0.001
NEPOCHS = 10
batch_size = 32

# Create the dataset
train_dataset = ImageDataset(df_train)

# Calculate class weights based on frequency
from torch.utils.data import WeightedRandomSampler

class_counts = df_train['label'].value_counts()  # Count samples in each class
class_weights = 1.0 / class_counts               # Inverse frequency of classes
sample_weights = df_train['label'].map(class_weights).values  # Assign weights to each sample

# Create a sampler for oversampling
sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)

# Create DataLoader using the sampler
train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler)

# Define a function to train the model
def train_model(model, loader, optimizer, num_epochs=NEPOCHS):
    # Initialize loss function
    criterion = nn.CrossEntropyLoss()
 
    # Training loop
    train_losses = []
    for epoch in tqdm.tqdm(range(num_epochs), total=num_epochs):
        running_loss = 0.0
        for i, data in enumerate(loader, 0):
            inputs, labels = data[0], data[1]
            # Zero the gradients
            optimizer.zero_grad()
            # Forward + backward + optimize
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
 
            # Collect losses
            running_loss += loss.item()
        epoch_loss = running_loss / len(loader)
        train_losses.append(epoch_loss)
 
    print('Training complete')
    return model, train_losses

# Initialize and train the model
model = BaselineCNN()
optimizer = optim.AdamW(model.parameters(), lr=learning_rate)
model, train_losses = train_model(model, train_loader, optimizer)

# Notes on results:
# Accuracy decreased compared to the baseline model without oversampling with WeightedRandomSampler, but the model performed slightly better for smaller classes.
# The accuracy of the dominant class slightly decreased.

# I also experimented with other models besides BaselineCNN:
# 1. ImprovedCNN model
#    - Final accuracy was 96.72%, which is slightly worse than the original BaselineCNN at 97.50%.
# 2. PretrainedResNet model
#    - Using a pretrained ResNet model resulted in an accuracy of 94.30%.

# Summary:
# - Using the BaselineCNN model yielded the best results, even compared to more complex models like ResNet.
# - The experiments demonstrated that more complex architectures do not always guarantee better performance, especially on smaller or specialized datasets.

